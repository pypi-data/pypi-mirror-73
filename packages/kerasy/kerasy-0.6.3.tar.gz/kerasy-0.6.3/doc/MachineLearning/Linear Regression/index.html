<!DOCTYPE html>
<html class="no-js" lang="en" >
  <head>
    <link rel="shortcut icon" href="https://iwasakishuto.github.io/images/apple-touch-icon/Kerasy.png" />

    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="author" content="Shuto" />
    <meta name="twitter:card" content="summary" />
    <meta property="og:url" content="https://iwasakishuto.github.io/Kerasy/doc//Users/iwasakishuto/Github/portfolio/Kerasy/MkDocs/site/MachineLearning/Linear Regression/index.html" />
    <meta property="og:title" content="Linear Regression" />
    <meta property="og:description" content="Brief explanation of Linear Regression and introduction of modules that can be used in Kerasy" />
    <meta property="og:image" content="https://iwasakishuto.github.io/images/FacebookImage/Kerasy.png" />
    <meta property="og:type" content="article" />

    <title>Linear Regression - Kerasy Documentation</title>
    <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css' />

    <link rel="stylesheet" type="text/css" href="https://iwasakishuto.github.io/Kerasy/doc/theme/css/custom.css" />
    <link rel="stylesheet" type="text/css" href="https://iwasakishuto.github.io/Kerasy/doc/theme/css/jupyter.css" />
    <link rel="stylesheet" type="text/css" href="https://iwasakishuto.github.io/Kerasy/doc/theme/css/theme_extra.css" />
    <link rel="stylesheet" type="text/css" href="https://iwasakishuto.github.io/Kerasy/doc/theme/css/theme.css" />

    <script>
      // Current page data
      var mkdocs_page_name = "Linear Regression";
      var mkdocs_page_input_path = "MachineLearning/Linear Regression.md";
      var mkdocs_page_url = "/Kerasy/MachineLearning/Linear Regression/";
    </script>
    <script src="https://iwasakishuto.github.io/Kerasy/doc/theme/js/jquery-2.1.1.min.js" defer></script>
    <script src="https://iwasakishuto.github.io/Kerasy/doc/theme/js/modernizr-2.8.3.min.js" defer></script>
    <script src="https://iwasakishuto.github.io/Kerasy/doc/theme/js/theme.js" defer></script>

    <!-- Common CSS in my portofolio  -->
    <link rel="stylesheet" type="text/css" href="https://iwasakishuto.github.io/css/jupyter.css" />
    <link rel="stylesheet" type="text/css" href="https://iwasakishuto.github.io/css/custom.css" media="screen" />
    <link rel="apple-touch-icon" sizes="152x152" href="https://iwasakishuto.github.io/images/apple-touch-icon/Kerasy.png" type="image/png" />
    <!-- Use fontawesome Icon -->
    <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.6.1/css/font-awesome.min.css" rel="stylesheet" type="text/css" />
    <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet" type="text/css" />
    <link href="https://use.fontawesome.com/releases/v5.7.0/css/all.css" rel="stylesheet" integrity="sha384-lZN37f5QGtY3VHgisS14W3ExzMWZxybE1SJSEsQp9S+oqd12jhcu+A56Ebc1zFSJ" crossorigin="anonymous" />
    <!-- Syntax highlight -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.6/styles/github.min.css" />
    <!-- Custom CSS -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.6/highlight.min.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>
    <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_CHTML"></script>
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"] ],
          displayMath: [ ['$$','$$'], ["\\[","\\]"] ]
        }
      });
      
    </script>
    <!-- Mermaid -->
    <script src="https://unpkg.com/mermaid/dist/mermaid.min.js" charset="UTF-8"></script>
    <script>
      mermaid.initialize({
        startOnLoad:true
      });
    </script>
    <script src="https://iwasakishuto.github.io/js/smooth-scroll.polyfills.min.js"></script>
  </head>
<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
      <div class="wy-side-nav-search">
        <a href="../.." class="icon icon-home"> Kerasy Documentation</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
	<ul class="tocbase current">
    
    
      


  <li class="navtree toctree-l1 inactive">
    <a class="" href="../..">Home</a>
  </li>
    
      
  <li class="navtree toctree-l1 label">
    <p class="caption">DeepLearning</p>
  </li>


  

  
    <li class="navtree toctree-l1 group">
      <ul class="navtree subnav-l1 current">
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../../DeepLearning/CNN/">CNN</a>
  </li>
        
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../../DeepLearning/Optimizers/">Optimizers</a>
  </li>
        
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../../DeepLearning/NeuralNetwork/">Neural Network</a>
  </li>
        
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../../DeepLearning/ComputationalGraph/">Computational Graph</a>
  </li>
        
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../../DeepLearning/Initializers/">Initializers</a>
  </li>
        
      </ul>
    </li>
    
      
  <li class="navtree toctree-l1 label">
    <p class="caption">MachineLearning</p>
  </li>


  

  
    <li class="navtree toctree-l1 group">
      <ul class="navtree subnav-l1 current">
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../Decomposition/">Decomposition</a>
  </li>
        
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../sampling/">Sampling</a>
  </li>
        
          


  
    
    <li class="navtree toctree-l2 page current">
      <a class="current" href="./">
        Linear Regression
          <span class="toctree-expand"></span>
      </a>
    </li>
    
      



      

  <li class="toctree-l2">
    <a href="#linear-regression-ridge-l2">
      Linear Regression (Ridge, L2)
      <span class="toctree-expand"></span>
    </a>
  </li>



      

  <li class="toctree-l2">
    <a href="#linear-regression-lasso-l1">
      Linear Regression (LASSO, L1)
      <span class="toctree-expand"></span>
    </a>
  </li>



      

  <li class="toctree-l2">
    <a href="#bayesian-linear-regression">
      Bayesian Linear Regression
      <span class="toctree-expand"></span>
    </a>
  </li>



      

  <li class="toctree-l2">
    <a href="#evidence-approximation-bayesian-regression">
      Evidence Approximation Bayesian Regression
      <span class="toctree-expand"></span>
    </a>
  </li>




  
        
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../Cluster/">Cluster</a>
  </li>
        
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../HMM/">HMM</a>
  </li>
        
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../EM%20algorithm/">EM algorithm</a>
  </li>
        
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../Tree/">Tree</a>
  </li>
        
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../Support%20Vector%20Machine/">Support Vector Machine</a>
  </li>
        
      </ul>
    </li>
    
      
  <li class="navtree toctree-l1 label">
    <p class="caption">BioInformatics</p>
  </li>


  

  
    <li class="navtree toctree-l1 group">
      <ul class="navtree subnav-l1 current">
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../../BioInformatics/Tandem%20Repeats/">Tandem Repeats</a>
  </li>
        
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../../BioInformatics/RefSeq/">RefSeq</a>
  </li>
        
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../../BioInformatics/Microarray/">Microarray</a>
  </li>
        
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../../BioInformatics/Secondary%20Structure/">Secondary Structure</a>
  </li>
        
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../../BioInformatics/Alignment/">Alignment</a>
  </li>
        
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../../BioInformatics/String%20Search/">String Search</a>
  </li>
        
      </ul>
    </li>
    
  </ul>
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="../..">Kerasy Documentation</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../..">Docs</a> &raquo;</li>
    
      
        
          <li>MachineLearning &raquo;</li>
        
      
    
    <li>Linear Regression</li>
    <li class="wy-breadcrumbs-aside">
      
        <a href="https://github.com/iwasakishuto/Kerasy"> Visit github &nbsp; <i class="fab fa-github"></i></a>
      
    </li>
  </ul>
  <hr/>
</div>
          <div role="main">
<div id="contents">
<div class="row">
    <div class="small-12 columns article">
        <div class="topRibbon">
            <h3>Linear Regression</h3>
        </div>
        <div class="row info-bar" style="margin-left:0rem;margin-bottom:6px;">
    <div class="small-12 columns">
        <ul class="inline-list">
            <li><span><i class="fa fa-calendar"></i>&nbsp; 2019-11-27(水)</span></li>
            <li><span><i class="far fa-file-code"></i>&nbsp; <a href="https://github.com/iwasakishuto/Kerasy/blob/gh-pages/kerasy/ML/linear.py">ML/linear.py</a></span></li>
            <!-- <li><span><i class="fa fa-folder-open"></i>&nbsp; <a href="https://iwasakishuto.github.io/Kerasy/doc/category/machinelearning.html">MachineLearning</a></span></li> -->
        </ul>
    </div>
</div>
<section class="article">
            <div class="section">
              
                <!-- SoC -->

<div class="admonition tip">
  <p class="admonition-title">Notebook</p>
  <p>Example Notebook: <a href="https://nbviewer.jupyter.org/github/iwasakishuto/Kerasy/blob/gh-pages/examples/linear.ipynb">Kerasy.examples.linear.ipynb</a></p>
</div>

<h2 id="linear-regression">Linear Regression</h2>
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">kerasy</span><span class="o">.</span><span class="n">ML</span><span class="o">.</span><span class="n">linear</span><span class="o">.</span><span class="n">LinearRegression</span><span class="p">(</span><span class="n">basis</span><span class="o">=</span><span class="s2">&quot;none&quot;</span><span class="p">,</span> <span class="o">**</span><span class="n">basisargs</span><span class="p">)</span>
</code></pre></div>

<p>The simplest model for regression is</p>

<div class="math">$$y(\mathbf{x},\mathbf{w}) = w_0 + w_1x_1 + \cdots + w_Dx_D$$</div>

<p>However, because of the linearity for the input variables <span class="math">\(x_i\)</span>, this model has a poor expressive ability. Therefore, we extend it by considering linear combinations of <strong>fixed nonlinear functions</strong> of the input variables, of the form</p>

<div class="math">$$y(\mathbf{x},\mathbf{w}) = \sum_{j=0}^{M-1}w_j\phi_j(\mathbf{x}) = \mathbf{w}^T\boldsymbol{\phi}(\mathbf{x})$$</div>

<p>In terms of the target variable <span class="math">\(t\)</span>, we assume that it is given by a <span class="math">\(y(\mathbf{x},\mathbf{w})\)</span> <strong>(deterministic)</strong> with additive <strong>Gaussian noise</strong> so that</p>

<div class="math">$$p(t|\mathbf{x},\mathbf{w},\beta) = \mathcal{N}\left(t|y(\mathbf{x},\mathbf{w}),\beta^{-1}\right)$$</div>

<p>Therefore, if we observe data, we obtain the following expression for the likelihood function</p>

<div class="math">$$
\begin{aligned}
p(\mathbf{t}|\mathbf{X},\mathbf{w},\beta) = \prod_{n=1}^N\mathcal{N}\left(t_n|\mathbf{w}^T\phi(\mathbf{x}_n),\beta^{-1}\right)\\
\begin{cases}
\begin{aligned}
\ln p(\mathbf{t}|\mathbf{X},\mathbf{w},\beta) &amp;=\sum_{n=1}^N\ln\mathcal{N}\left(t_n|\mathbf{w}^T\phi(\mathbf{x}_n),\beta^{-1}\right)\\
&amp;= \frac{N}{2}\ln\beta - \frac{N}{2}\ln(2\pi) - \beta E_D(\mathbf{w})\\
E_D(\mathbf{w}) &amp;= \frac{1}{2}\sum_{n=1}^N\left\{t_n - \mathbf{w}^T\phi(\mathbf{x}_n)\right\}^2
\end{aligned}
\end{cases}
\end{aligned}
$$</div>

<p>As we obtain such a likelihood, it is easy to maximize them respect to <span class="math">\(\mathbf{w}\)</span> and <span class="math">\(\beta\)</span>. (Setting the gradient to zero.)</p>

<div class="math">$$
\begin{aligned}
\nabla\ln p(\mathbf{t}|\mathbf{w},\beta) &amp;= \sum_{n=1}^N\left\{t_n - \mathbf{w}^T\phi(\mathbf{x}_n)\right\}\phi(\mathbf{x}_n)^T = 0\\
0 &amp;= \sum_{n=1}^Nt_n\phi(\mathbf{x}_n)^T - \mathbf{w}^T\left(\sum_{n=1}^N\phi(\mathbf{x}_n)\phi(\mathbf{x}_n)^T\right)\\
\therefore\mathbf{w}_{\text{ML}} &amp;= \left(\Phi^T\Phi\right)^{-1}\Phi^T\mathbf{t}\\
\frac{1}{\beta_{\text{ML}}} &amp;= \frac{1}{N}\sum_{n=1}^N\left\{t_n -\mathbf{w}_{\text{ML}}^T\phi(\mathbf{x}_n)\right\}^2
\end{aligned}
$$</div>

<ul>
<li><span class="math">\(\Phi\)</span> is an <span class="math">\(N\times M\)</span> matrix, called the <strong>design matrix</strong>, whose elements are given by <span class="math">\(\Phi_{nj} = \phi_j(\mathbf{x}_n)\)</span></li>
<li>The quantity <span class="math">\(\Phi^{\dagger}\equiv\left(\Phi^T\Phi\right)^{-1}\Phi^T\)</span> is known as the <font color="red"><b>Moore-Penrose pseudo-inverse</b></font> of the matrix <span class="math">\(\Phi\)</span></li>
</ul>

<h2 id="linear-regression-ridge-l2">Linear Regression (Ridge, L2)</h2>
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">kerasy</span><span class="o">.</span><span class="n">ML</span><span class="o">.</span><span class="n">linear</span><span class="o">.</span><span class="n">LinearRegressionRidge</span><span class="p">(</span><span class="n">lambda_</span><span class="p">,</span> <span class="n">basis</span><span class="o">=</span><span class="s2">&quot;none&quot;</span><span class="p">,</span> <span class="o">**</span><span class="n">basisargs</span><span class="p">)</span>
</code></pre></div>

<p>As you see at the <a href="https://nbviewer.jupyter.org/github/iwasakishuto/Kerasy/blob/gh-pages/examples/linear.ipynb">notebook</a>, it is likely to <font color="red"><b>over-fit</b></font> especially when model is too complicated. (<span class="math">\(M\rightarrow\text{Large}\)</span>) For avoiding it, we introduce the idea of <strong>adding a regularization term to an error function.</strong></p>

<div class="math">$$E_D(\mathbf{w}) + \lambda E_{\mathbf{w}}(\mathbf{w})$$</div>

<p>where <span class="math">\(\lambda\)</span> is the regularization coefficient that controls the relative importance of the data-dependent error <span class="math">\(E_D(\mathbf{w})\)</span> and the regularization term <span class="math">\(E_{\mathbf{w}}(\mathbf{w})\)</span>.</p>

<p>One of the simplest forms of regularizer is given by</p>

<div class="math">$$E_{\mathbf{w}}(\mathbf{w}) = \frac{1}{2}\mathbf{w}^T\mathbf{w}$$</div>

<p>Then, the total error function and Maximum likelihood estimated <span class="math">\(\mathbf{w}\)</span> is expressed as</p>

<div class="math">$$
\frac{1}{2}\sum_{n=1}^N\left\{t_n- \mathbf{w}^T\phi(\mathbf{x}_n)\right\}^2 + \frac{\lambda}{2}\mathbf{w}^T\mathbf{w}\\
\mathbf{w}_{\text{ML}} = \left(\lambda\mathbf{I} + \Phi^T\Phi\right)^{-1}\Phi^T\mathbf{t}
$$</div>

<h2 id="linear-regression-lasso-l1">Linear Regression (LASSO, L1)</h2>
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">kerasy</span><span class="o">.</span><span class="n">ML</span><span class="o">.</span><span class="n">linear</span><span class="o">.</span><span class="n">LinearRegressionLASSO</span><span class="p">(</span><span class="n">lambda_</span><span class="p">,</span> <span class="n">basis</span><span class="o">=</span><span class="s2">&quot;none&quot;</span><span class="p">,</span> <span class="o">**</span><span class="n">basisargs</span><span class="p">)</span>
</code></pre></div>

<p>Consider the regularizer given by</p>

<div class="math">$$E_{\mathbf{w}}(\mathbf{w}) = \|\mathbf{w}\|$$</div>

<p>This function is not differentiable, so we use <font color="red"><b>ADMM(Alternating Direction Method of Multipliers)</b></font>. In this algorithm, we use <font color="red"><b>Extended Lagrange multiplier</b></font>.</p>

<div class="math">$$\begin{aligned}
L_{\rho}\left(\mathbf{w},\mathbf{z},\boldsymbol{\alpha}\right) &amp;= \frac{1}{2}\|\mathbf{y} - \mathbf{Xw}\|^2 + \lambda|\mathbf{z}| + \boldsymbol{\alpha}^T\left(\mathbf{w}-\mathbf{z}\right) + \frac{\rho}{2}\|\mathbf{w}-\mathbf{z}\|^2\\
\text{subject to  }\mathbf{w} &amp;= \mathbf{z}
\end{aligned}$$</div>

<p>We minimize <span class="math">\(L_{\rho}\)</span> respect to <span class="math">\(\mathbf{w}\)</span> and <span class="math">\(\mathbf{z}\)</span> <strong>repeatedly</strong>.</p>

<div class="math">$$
\begin{cases}
  \begin{aligned}
    \mathbf{w}&amp;\leftarrow\underset{\mathbf{w}}{\text{argmin}}L_{\rho}\left(\mathbf{w},\mathbf{z},\boldsymbol{\alpha}\right)\text{ with fixed with $\mathbf{z},\boldsymbol{\alpha}$}\\
    &amp;=\left(\mathbf{X}^T\mathbf{X} + \rho\mathbf{I}\right)^{-1}\left(\mathbf{X}^T\mathbf{y} - \boldsymbol{\alpha} + \rho\mathbf{z}\right)\\
    \mathbf{z}&amp;\leftarrow\underset{\mathbf{z}}{\text{argmin}}L_{\rho}\left(\mathbf{w},\mathbf{z},\boldsymbol{\alpha}\right)\text{ with fixed with $\mathbf{w},\boldsymbol{\alpha}$}\\
    &amp;= \text{prox}_{\frac{\lambda}{\rho}|\ast|}\left(w_i + \frac{\alpha_i}{\rho}\right)\\
    \boldsymbol{\alpha}&amp;\leftarrow\boldsymbol{\alpha} + \rho\left(\mathbf{w}-\mathbf{z}\right)
  \end{aligned}
\end{cases}\\
\text{prox}_{c|\ast|}(z_0) := \underset{z}{\text{argmin}}\left\{c|z| + \frac{1}{2}\left(z-z_0\right)^2\right\} = \begin{cases}z_0-c &amp; (c &lt; z_0)\\0 &amp; (-c\leq z_0 \leq c)\\z_0 + c &amp; (z_0 &lt; -c)\end{cases}
$$</div>

<p><hr>
<p>A more general regularizer is sometimes used, for which the regularized error takes the form</p>
<div class="math">$$\frac{1}{2}\sum_{n=1}^N\left{t_n-\mathbf{w}^T\phi(\mathbf{x}<em>n)\right}^2 + \frac{\lambda}{2}\sum</em>{j=1}^M|w_j|^q$$</div>
<hr></p>
<h2 id="bayesian-linear-regression">Bayesian Linear Regression</h2>
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">kerasy</span><span class="o">.</span><span class="n">ML</span><span class="o">.</span><span class="n">linear</span><span class="o">.</span><span class="n">BayesianLinearRegression</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span> <span class="n">basis</span><span class="o">=</span><span class="s2">&quot;none&quot;</span><span class="p">,</span> <span class="o">**</span><span class="n">basisargs</span><span class="p">)</span>
</code></pre></div>

<p>We turn to a <strong>Bayesian treatment</strong> of linear regression, which will <strong>avoid the over-fitting problem</strong> of maximum likelihood, and which will also lead to <strong>automatic methods</strong> of determining model complexity <strong>using the training data alone.</strong> (Not requiring <strong>Hold-out methods</strong>)</p>

<ol>
<li>We assume that the prior distribution of <span class="math">\(\mathbf{w}\)</span> is given by a Gaussian distribution of the form
<div class="math">$$p(\mathbf{w}|\alpha) = \mathcal{N}\left(\mathbf{w}|\mathbf{0},\alpha^{-1}\mathbf{I}\right)$$</div>
</li>
<li>As I mentioned before, likelihood is given by
<div class="math">$$p(t|\mathbf{x},\mathbf{w},\beta) = \mathcal{N}\left(t|y(\mathbf{x},\mathbf{w}),\beta^{-1}\right)\\
p(\mathbf{t}|\mathbf{X},\mathbf{w},\beta) = \prod_{n=1}^N\mathcal{N}\left(t_n|\mathbf{w}^T\phi(\mathbf{x}_n),\beta^{-1}\right)$$</div>
</li>
<li>From the Bayes' theorem, <strong>posterior distribution is proportional to the product of prior distribution and likelihood</strong>. Therefore, posterior distribution is described as
<div class="math">$$p(\mathbf{w}|\mathbf{t}) = \mathcal{N}\left(\mathbf{m}_N,\mathbf{S}_N\right)\\
\begin{cases}\mathbf{m}_N &amp;= \beta\mathbf{S}_N\Phi^T\mathbf{t}\\\mathbf{S}_N^{-1} &amp;= \alpha\mathbf{I} + \beta\Phi^T\Phi\end{cases}$$</div>
</li>
<li>Finally, the <strong>predictive distribution</strong> defined by:
<div class="math">$$\begin{aligned}
p(t|\mathbf{t},\alpha,\beta) &amp;= \int p(t|\mathbf{w},\beta)p(\mathbf{w}|\mathbf{t},\alpha,\beta)d\mathbf{w}\\
&amp;= \mathcal{N}\left(t|\mathbf{m}_N^T\phi(\mathbf{x}),\sigma^2_N(\mathbf{x})\right)\\
\sigma^2_N(\mathbf{x}) &amp;= \frac{1}{\beta} + \phi(\mathbf{x})^T\mathbf{S}\phi(\mathbf{x})
\end{aligned}$$</div>
</li>
</ol>

<div class="admonition summary">
  <p class="admonition-title">Reference</p>
  <p>If you want to know the derivation process (calculation process) in detail, please visit <a href="https://iwasakishuto.github.io/University/3A/生物データマイニング論-4.html">here</a>.</p>
</div>

<h2 id="evidence-approximation-bayesian-regression">Evidence Approximation Bayesian Regression</h2>
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">kerasy</span><span class="o">.</span><span class="n">ML</span><span class="o">.</span><span class="n">linear</span><span class="o">.</span><span class="n">EvidenceApproxBayesianRegression</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">basis</span><span class="o">=</span><span class="s2">&quot;none&quot;</span><span class="p">,</span> <span class="o">**</span><span class="n">basisargs</span><span class="p">)</span>
</code></pre></div>

<p>In a fully Bayesian treatment of the linear basis function model, we would <strong>introduce prior distributions over the hyperparameters <span class="math">\(\alpha\)</span>, and <span class="math">\(\beta\)</span>.</strong></p>

<div class="math">$$
\begin{aligned}
p(t|\mathbf{t},\mathbf{X},\mathbf{x},\alpha,\beta)
&amp;= \int p(t|\mathbf{w},\mathbf{x},\beta)p(\mathbf{w}|\mathbf{t},\mathbf{X},\alpha,\beta)d\mathbf{w}\\
&amp;\Rightarrow \int p(t|\mathbf{w},\mathbf{x},\beta)p(\mathbf{w}|\mathbf{t},\mathbf{X},\alpha,\beta)p(\alpha,\beta|\mathbf{t},\mathbf{X})d\mathbf{w}d\alpha d\beta \quad (\ast)
\end{aligned}
$$</div>

<p>Marginalize with respect to these hyperparameters as well as with respect to the parameters <span class="math">\(\mathbf{w}\)</span> to make predictions.</p>

<p>As, the <strong>complete marginalization over all of these variables is analytically intractable</strong>, We will maximize <span class="math">\((\ast)\)</span> in line with the framework of <font color="red"><b>empirical Bayes</b></font> (or <strong>type 2 maximum likelihood</strong>, <strong>generalized maximum likelihood</strong>, <strong>evidence approximation</strong>)</p>

<p>In the framework, we repeat the following process.</p>

<ol>
<li>Obtain the marginal likelihood function by first integrating over the parameters <span class="math">\(\mathbf{w}\)</span>
<div class="math">$$p(\mathbf{t}|\alpha,\beta) = \int p(\mathbf{t}|\mathbf{w},\mathbf{X},\beta)p(\mathbf{w}|\alpha)d\mathbf{w}$$</div>
</li>
<li>Maximize <span class="math">\(p(\mathbf{t}|\alpha,\beta)\)</span> with respect to <span class="math">\(\alpha\)</span> and <span class="math">\(\beta\)</span>.
<div class="math">$$
\begin{cases}
  \begin{aligned}
    \gamma &amp;= \sum_{i}\frac{\lambda_i}{\lambda_i + \alpha^{\text{old}}}\\
    \alpha^{\text{new}} &amp;= \frac{\gamma}{\mathbf{m}_N^T\mathbf{m}_N}\\
    \frac{1}{\beta^{\text{new}}} &amp;= \frac{1}{N-\gamma}\sum_{n=1}^N\left\{t_n - \mathbf{m}_N^T\phi(\mathbf{x}_n)\right\}^2
  \end{aligned}
\end{cases}
$$</div>
</li>
</ol>

<div class="admonition reference">
  <p class="admonition-title">Reference</p>
  <ul>
    <li><a href="https://www.microsoft.com/en-us/research/people/cmbishop/#!prml-book">Pattern Recognition and Machine Learning by Christopher Bishop</a></li>
  </ul>
</div>

<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        fonts: [['STIX', 'TeX']]," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>

<!-- EoC -->

<p></section>
    </div>
</div>
</div></p>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../Cluster/" class="btn btn-neutral float-right" title="Cluster">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href="../sampling/" class="btn btn-neutral" title="Sampling"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="http://www.mkdocs.org">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
      
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" style="cursor: pointer">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
      
        <span><a href="../sampling/" style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
        <span style="margin-left: 15px"><a href="../Cluster/" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>
    <script>var base_url = '../..';</script>
    <script src="../../js/theme.js" defer></script>
      <script src="../../search/main.js" defer></script>

</body>
</html>
