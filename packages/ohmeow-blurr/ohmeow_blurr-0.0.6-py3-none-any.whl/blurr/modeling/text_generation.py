# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/02e_modeling-text-generation.ipynb (unless otherwise specified).

__all__ = ['text_gen_splitter', 'HF_TextGenModelCallback']

# Cell
import ast, torch
from transformers import *
from fastai2.text.all import *

from ..data.all import *
from .core import *

# Cell
def text_gen_splitter(m, arch):
    """Custom param splitter for text generation models"""
    model = m.hf_model if (hasattr(m, 'hf_model')) else m

    if (arch == 'bart'):
        groups = L(model.model.encoder,
                   model.model.decoder.embed_tokens,
                   model.model.decoder.embed_positions,
                   model.model.decoder.layers,
                   model.model.decoder.layernorm_embedding)

        return groups.map(params).filter(lambda el: len(el) > 0)

    raise ValueError('Invalid architecture')

# Cell
class HF_TextGenModelCallback(HF_BaseModelCallback):
    def after_pred(self):
        if ('labels' in self.xb[0]):
            self.hf_loss, self.learn.pred = self.pred[0], self.pred[1]
        else:
            self.learn.pred = self.pred[0]

    def after_loss(self): self.learn.loss = self.hf_loss

# Cell
@typedispatch
def show_results(x:HF_TextGenerationInput, y, samples, outs, hf_tokenizer, generated_max_len=130,
                 ctxs=None, max_n=6, **kwargs):

    res = L([ (sample[0], sample[1], pred[0].replace(hf_tokenizer.pad_token, '')[:generated_max_len])
             for sample, pred in zip(samples, outs) ])

    display_df(pd.DataFrame(res, columns=['text', 'target', 'prediction'])[:max_n])
    return ctxs

# Cell
@patch
def generate_text(self:Learner, inp, max_length=130, min_length=30, **kwargs):
    """Uses the built-in `generate` method to generate the text
    (see [here](https://huggingface.co/transformers/main_classes/model.html#transformers.PreTrainedModel.generate)
    for a list of arguments you can pass in)
    """
    # grab the huggingface tokenizer from the learner's dls.tfms
    hf_textblock_tfm = self.dls.tfms[0]
    hf_tokenizer = hf_textblock_tfm.hf_tokenizer
    tok_kwargs = hf_textblock_tfm.tok_kwargs

    input_ids = hf_tokenizer.encode(inp, padding=True, truncation=True, return_tensors='pt', **tok_kwargs)
    input_ids = input_ids.to(self.model.hf_model.device)

    gen_text = self.model.hf_model.generate(input_ids, max_length=max_length, min_length=min_length, **kwargs)

    outputs = [ hf_tokenizer.decode(txt, skip_special_tokens=True, clean_up_tokenization_spaces=False)
               for txt in gen_text ]

    return outputs