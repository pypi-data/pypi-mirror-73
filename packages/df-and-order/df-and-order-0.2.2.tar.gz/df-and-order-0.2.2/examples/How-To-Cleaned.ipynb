{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# df-and-order how-to!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is df-and-order anyway?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `df-and-order` your interactions with dataframes become very clean and predictable.\n",
    "\n",
    "Say you've been working on some project for one month already and you had a bunch of experiments. \n",
    "\n",
    "Your working directory ended up like this:\n",
    "\n",
    "    data/\n",
    "    ├── raw_df_proj1.csv\n",
    "    ├── raw_df_new_prj1.csv\n",
    "    ├── cleaned_df_v1.csv\n",
    "    ├── cleaned_df_the_best.csv\n",
    "    ├── cleaned_df.csv\n",
    "    └── cleaned_df_improved.csv\n",
    "Looks familiar? :) Except the namings it would be challenging to find how exactly those files were generated. How to reproduce the result? It'd be feasible to find the roots ( at least if you use some VCS ) yet very time-consuming.\n",
    "\n",
    "`df-and-order` was made to tackle these problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In every task it always starts with some intial, commonly raw dataframe. It could be some logs, backend table etc. Then we come to play with it, transform it somehow to finally get a nice&clean dataframe. \n",
    "\n",
    "`df-and-order` assigns a config file to every raw dataframe. The config will contain all the useful metadata and more importantly: declaration of every transformation performed on the dataframe. Just by looking at the config file we would be able to say how some transformation was done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`df-and-order` assumes that you already have a dataframe to work with. ( unfortunately it can't provide it for you... )\n",
    "\n",
    "The only thing the lib wants you to do is to organize your dataframes in separate folders. The lib is config-based so it's nice to have a folder that contains all at once:\n",
    "\n",
    "- the initial dataframe \n",
    "\n",
    "- a config for it \n",
    "\n",
    "- all transformed variations of the initial dataframe.\n",
    "\n",
    "You should pick a unique identifier for each dataframe, it will serve as the folder name and the filename for the initial dataframe.\n",
    "\n",
    "Example of such structure:\n",
    "\n",
    "    data/\n",
    "    ├── unique_df_id_1/ - folder with all artifacts for a df with id unique_df_id_1\n",
    "    │   ├── unique_df_id_1.csv - initial dataframe\n",
    "    │   ├── df_config.yaml - contains metadata and declared transformations\n",
    "    │   ├── transform_1_unique_df_id_1.csv - first transformed df\n",
    "    │   └── transform_2_unique_df_id_1.csv - second transformed df\n",
    "    ├── unique_df_id_2/ - same goes with other dataframes\n",
    "    │   ├── ...\n",
    "    │   └── ...\n",
    "    └── unique_df_id_3/\n",
    "        ├── ...\n",
    "        ├── ...\n",
    "        ├── ...\n",
    "        └── ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. We need a dataframe!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to create it by hand!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_df = pd.DataFrame({\n",
    "    'num_col': [1,2,3,4,5],\n",
    "    'str_col': ['one', 'two', 'three', 'four', 'five'],\n",
    "    'date_col': ['2020-05-17', '2020-05-18', '2020-05-19', '2020-05-20', '2020-05-21'],\n",
    "    'redundant_col': [0, 0, 0, 0, 0]\n",
    "})\n",
    "example_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What an amazing dataframe we have! Let's choose an id for our dataframe. It can be anything, but unique in your data folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_df_id = 'super_demo_df_2020'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a folder for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "df_folder_path = os.path.join('data', example_df_id)\n",
    "if not os.path.exists(df_folder_path):\n",
    "    os.makedirs(df_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only thing left is to save our dataframe there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = example_df_id + '.csv'\n",
    "example_df.to_csv(os.path.join(df_folder_path, filename), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -l data/$example_df_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hooray! Next step is to create a config file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Config file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Config file contains all metadata we find useful and all transformations needed as well.\n",
    "\n",
    "`DfReader` operates in your data folder and knows where to locate all dataframes and configs for them. We will create new config using `DfReader` instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# in case you've cloned the repo without installing the lib via pip\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from df_and_order.df_reader import DfReader\n",
    "from df_and_order.df_cache import DfCache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DfReader is able to work with any format you want by using `DfCache` subclasses. Each subclass provides logic how to save/load a dataframe. \n",
    "\n",
    "See the example below, where we create simple pandas wrapper for saving/loading csv files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CsvDfCache(DfCache):\n",
    "    # just a basic wrapper around pandas csv built-in methods.\n",
    "    def _save(self, df: pd.DataFrame, path: str, *args, **kwargs):\n",
    "        df.to_csv(path, index=False, *args, **kwargs)\n",
    "\n",
    "    def _load(self, path: str, *args, **kwargs) -> pd.DataFrame:\n",
    "        return pd.read_csv(path, *args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as I mentioned earlier, we first need an instance of `DfReader`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we must declare which format our dataframes saved in\n",
    "df_format = 'csv'\n",
    "# can be any path you want, in our case it's 'data' folder\n",
    "dir_path = 'data/'\n",
    "reader = DfReader(dir_path=dir_path, format_to_cache_map={\n",
    "    # DfReader now knows how to work with csv files.\n",
    "    df_format: CsvDfCache()\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are all set for now and ready to create a config!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you may want to provide any additional information for describing a dataset\n",
    "# here, as an example, we save the info about the dataset's author\n",
    "metadata = {'author': 'Data Man'}\n",
    "# the unique id we came up with above. \n",
    "df_id = example_df_id\n",
    "# other information is already available for us\n",
    "reader.create_df_config(df_id=df_id, # config will store dataframe id as well\n",
    "                        initial_df_format=df_format, # in which format initial dataframe is saved\n",
    "                        metadata=metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done! let's take a look at the config file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat data/$example_df_id/df_config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple as that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Reading a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader.read(df_id=df_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I started the section with the code right away because it's so simple and intuitive, no need for comments! :)\n",
    "\n",
    "You just tell `DfReader` a dataframe id and you get the dataframe right back. No more hardcoded paths and mixed up formats. Once you set up `DfReader` - everything just works. \n",
    "\n",
    "Close your eyes and imagine how beneficial it is when working in the same repository with many fellow colleagues. No more shared notebooks with hardcoded paths leading to who-knows-how generated dataframes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Still not convinced df-and-order is useful? Just watch!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a good idea to hide all the logic behind your own subclass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AmazingDfReader(DfReader):\n",
    "    def __init__(self):\n",
    "        # while working in some repo, our data is usually stored in some specific\n",
    "        # place we can provide a path for. Ideally you should write some path generator\n",
    "        # to be able to run the code from any place in your repository.\n",
    "        dir_path = 'data'\n",
    "        reader = super().__init__(dir_path=dir_path, format_to_cache_map={\n",
    "            # here we list all the formats we want to work with\n",
    "            'csv': CsvDfCache()\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enjoy the next cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazing_reader = AmazingDfReader()\n",
    "amazing_reader.read(df_id=df_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you see how cool it is? Anybody can use AmazingDfReader across the codebase in a super clean way without bothering how it's configured!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very often our initial dataframe is the raw one and needs to be transformed in some way. \n",
    "\n",
    "e.g. we want still need the initial dataframe since it contains some important information, nonetheless we can't use it to fit our model. No doubt, it requires some changes.\n",
    "\n",
    "`df-and-order` supports `in-memory` transformations as well as `permanent` ones. The only difference is that in the permanent case we store the resulting dataframe on disk next to the initial df. \n",
    "\n",
    "You can see a transformation as a combination of one or many steps.\n",
    "\n",
    "e.g. we may want to:\n",
    "\n",
    "    - first drop column 'redundant_col'\n",
    "    - then convert column 'date_col' from str to date\n",
    "    Do it all in memory only\n",
    "\n",
    "Behind the scenes each step represents a class with the only one method called `transform`. It takes a df and returns a df. Here's the intuitive example:\n",
    "\n",
    "    class DropColsTransformStep(DfTransformStep):\n",
    "        \"\"\"\n",
    "        Simply drops some undesired columns from a dataframe.\n",
    "        \"\"\"\n",
    "        def __init__(self, cols: List[str]):\n",
    "            self._cols_to_drop = cols\n",
    "\n",
    "    def transform(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        return df.drop(self._cols_to_drop, axis=1)\n",
    "\n",
    "Then we wrap it in the `DfTransformStepConfig` class that doesn't perform the transformation but rather just describes the step:\n",
    "\n",
    "The easiest way to initialize `DfTransformStepConfig` is by passing `DfTransformStep` subclass type along with the init parameters:\n",
    "\n",
    "    DfTransformStepConfig.from_step_type(step_type=DropColsTransformStep,\n",
    "                                         params={'cols': ['redundant_col']}),\n",
    "                                         \n",
    "Important note here:\n",
    "\n",
    "`DfTransformStep` suclass should be stored in the separate python file, not in some notebook etc. \n",
    "\n",
    "Otherwise, `df-and-order` will not be able to locate it.\n",
    "                                         \n",
    "Another way is to provide the full module path for your `DfTransformStep` suclass, including the class name. Choose whatever suits you.\n",
    "\n",
    "    DfTransformStepConfig(module_path='df_and_order.steps.DropColsTransformStep',\n",
    "                          params={'cols': ['redundant_col']}),\n",
    "\n",
    "In both cases `params` will be passed to init method of the specified `DfTransformStep` suclass.\n",
    "\n",
    "All the transforms declarations will be translated to the config file. \n",
    "\n",
    "If it feels overwhelming, just follow the following example and everything will become clear:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to remove `redundant_col` since it doesn't provide any useful information and we also need to convert `date_col` to datetime. Since our dataframe is quite small, we will do all the transformations in memory, without any intermediates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from df_and_order.df_transform import DfTransformConfig\n",
    "from df_and_order.df_transform_step import DfTransformStepConfig\n",
    "from df_and_order.steps.pd import DropColsTransformStep, DatesTransformStep\n",
    "\n",
    "# we describe all the steps required\n",
    "in_memory_steps = [\n",
    "    DfTransformStepConfig.from_step_type(step_type=DropColsTransformStep,\n",
    "                                         params={'cols': ['redundant_col']}),\n",
    "    DfTransformStepConfig.from_step_type(step_type=DatesTransformStep,\n",
    "                                         params={'cols': ['date_col']})\n",
    "]\n",
    "\n",
    "# arbitrary unique id for our transformation\n",
    "example_transform_id = 'model_input'\n",
    "# here's the instance of our entire transform\n",
    "example_transform = DfTransformConfig(transform_id=example_transform_id, \n",
    "                                      df_format=df_format,\n",
    "                                      in_memory_steps=in_memory_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_df = amazing_reader.read(df_id=df_id, \n",
    "                                     transform=example_transform)\n",
    "transformed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pretty rad, isn't it?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our transform is now visible in the config:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat data/$example_df_id/df_config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Note: you are free to edit the config file manually as well!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once a transform is declared in the config file you can just pass `transform_id` to the `DfReader.read` method. See:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazing_reader.read(df_id=df_id, transform_id=example_transform_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe you want to switch to your initial dataframe? No problem! Just don't pass `transform_id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_df = amazing_reader.read(df_id=df_id)\n",
    "initial_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's cover the case when we want to persist a transform's result. It's a good idea to remove `redundant_col` once and for all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we describe all the steps required\n",
    "in_memory_steps = [\n",
    "    DfTransformStepConfig.from_step_type(step_type=DatesTransformStep,\n",
    "                                         params={'cols': ['date_col']})\n",
    "]\n",
    "\n",
    "# let's just move DropColsTransformStep from in_memory to permanent steps\n",
    "permanent_steps = [\n",
    "    DfTransformStepConfig.from_step_type(step_type=DropColsTransformStep,\n",
    "                                     params={'cols': ['redundant_col']}),\n",
    "]\n",
    "\n",
    "# arbitrary unique id for our transformation\n",
    "permanent_transform_id = 'model_input_permanent'\n",
    "# here's the instance of our entire transform\n",
    "permanent_transform = DfTransformConfig(transform_id=permanent_transform_id, \n",
    "                                        df_format=df_format,\n",
    "                                        in_memory_steps=in_memory_steps,\n",
    "                                        permanent_steps=permanent_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = amazing_reader.read(df_id=df_id, \n",
    "                               transform=permanent_transform)\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat data/$example_df_id/df_config.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -l data/$example_df_id/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we now have `model_input_permanent_super_demo_df_2020.csv` file stored to the disk.\n",
    "\n",
    "Every time after calling `read` with the transform_id - it recovers from the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazing_reader.read(df_id=df_id, \n",
    "                    transform=permanent_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important note: `in-memory` transforms run everytime when your read a dataframe, no matter it was stored on the disk or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### That's it, now you are ready to try df-and-order power in your own projects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some advanced stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reacting to changes in transformations codebase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, even after having all the transformation steps declared in the config file, it doesn't prevent us from code changes in those steps subclasses. Once a step is changed, we have an outdated transformed dataframe on the disk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`df-and-order` has a built-in safety mechanism for avoiding such cases.\n",
    "\n",
    "It compares the creation date of the persisted dataframe with the last modification date of any of the permanent steps. Meaning if a permanent step we used to transform the dataframe was changed afterwards - we can no longer use it. It's crucial while working in the same repo with others. All your team members must read the same dataframe using the same config."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from example_steps.steps import DummyTransformStep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat example_steps/steps.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transform above does literally nothing, but bear with me."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "permanent_steps = [\n",
    "    DfTransformStepConfig.from_step_type(step_type=DummyTransformStep, params={})\n",
    "]\n",
    "dummy_transform_id = 'dummy'\n",
    "dummy_transform = DfTransformConfig(transform_id=dummy_transform_id, \n",
    "                                    df_format=df_format,\n",
    "                                    permanent_steps=permanent_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazing_reader.read(df_id=df_id, \n",
    "                    transform=dummy_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat data/super_demo_df_2020/df_config.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -l data/super_demo_df_2020/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nothing new so far. But now let's change the transform step file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('example_steps/steps.py', \"a\") as file:\n",
    "    file.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we then try to read the transformed dataframe - it crashes since the code of our dummy step was modified after the dataframe was persisted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazing_reader.read(df_id=df_id, transform_id=dummy_transform_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two ways to deal with it. \n",
    "\n",
    "First one is to force the read operation by passing `forced=True`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazing_reader.read(df_id=df_id, transform_id=dummy_transform_id, forced=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can save you time when you are sure that your data will be consistent with your expectations yet this way is certainly not recommended.\n",
    "\n",
    "Yeah, it can be annoying to get such an error after some minor changes, e.g. something was renamed or blank lines were removed.\n",
    "\n",
    "But it's better to get an error rather than outdated wrong dataframe.\n",
    "If we remove the file and try again - everything works just fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm data/$example_df_id/dummy_super_demo_df_2020.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazing_reader.read(df_id=df_id, transform_id=dummy_transform_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note on in-memory transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your transform consists of both in-memory and permanent steps, your in-memory steps are not allowed to change the shape of df. Remember, in-memory steps are applied every time your read a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# made up example when we remove some cols in memory and then perform \n",
    "# some permanent transform step that will cause our dataframe to be persisted\n",
    "in_memory_steps = [\n",
    "    DfTransformStepConfig.from_step_type(step_type=DropColsTransformStep,\n",
    "                                     params={'cols': ['redundant_col']}),\n",
    "]\n",
    "permanent_steps = [\n",
    "    DfTransformStepConfig.from_step_type(step_type=DatesTransformStep,\n",
    "                                         params={'cols': ['date_col']})\n",
    "]\n",
    "\n",
    "# arbitrary unique id for our transformation\n",
    "bad_in_memory_transform_id = 'bad_in_memory'\n",
    "# here's the instance of our entire transform\n",
    "bad_in_memory_transform = DfTransformConfig(transform_id=bad_in_memory_transform_id, \n",
    "                                            df_format='csv',\n",
    "                                            in_memory_steps=in_memory_steps,\n",
    "                                            permanent_steps=permanent_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazing_reader.read(df_id=df_id, transform=bad_in_memory_transform)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
